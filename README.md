# Handwritten English Character Classification (Chars74K + ResNet-18)

## ğŸ“Œ MÃ´ táº£

BÃ i toÃ¡n phÃ¢n loáº¡i kÃ½ tá»± tiáº¿ng Anh dáº¡ng viáº¿t in/viáº¿t thÆ°á»ng gá»“m:  
**0â€“9, Aâ€“Z, aâ€“z** (tá»•ng tá»‘i Ä‘a 62 lá»›p).

- **Dataset:** Chars74K â€“ Digital English Font  
  <https://www.kaggle.com/datasets/supreethrao/chars74kdigitalenglishfont>
- **Model:** CNN â€“ ResNet-18 (tÃ¹y chá»‰nh cho áº£nh grayscale 64Ã—64)

---

## ğŸ“Š Káº¿t quáº£ Training

### Cáº¥u hÃ¬nh thÃ­ nghiá»‡m
- **Tá»•ng sá»‘ áº£nh:** 62,992
- **Train/Val/Test:** 70%/20%/10% = 44,094 / 12,598 / 6,300
- **Batch size:** 64
- **Epochs:** 20
- **Learning rate:** 1e-3
- **Optimizer:** Adam
- **Image size:** 64Ã—64
- **Device:** CUDA (GPU)

### Hiá»‡u suáº¥t mÃ´ hÃ¬nh

| Metric | GiÃ¡ trá»‹ |
|--------|---------|
| **Final Test Accuracy** | **91.33%** |
| **Best Validation Accuracy** | 91.77% (Epoch 18) |
| **Final Training Loss** | 0.1106 |

### Training Progress

| Epoch | Train Loss | Val Accuracy |
|-------|-----------|--------------|
| 1/20  | 0.5799    | 84.93%       |
| 5/20  | 0.2547    | 88.93%       |
| 10/20 | 0.1846    | 90.10%       |
| 15/20 | 0.1409    | 91.73%       |
| 20/20 | 0.1106    | 90.78%       |

**Nháº­n xÃ©t:**
- Model há»™i tá»¥ tá»‘t sau 20 epochs
- Validation accuracy Ä‘áº¡t Ä‘á»‰nh ~91.77% á»Ÿ epoch 18
- CÃ³ dáº¥u hiá»‡u overfitting nháº¹ (val acc giáº£m tá»« epoch 18â†’20)
- Training loss giáº£m Ä‘á»u Ä‘áº·n tá»« 0.5799 â†’ 0.1106

---

## ğŸš€ Flow & Giáº£i thÃ­ch nhanh (Training, Evaluation) 

DÆ°á»›i Ä‘Ã¢y lÃ  mÃ´ táº£ ngáº¯n gá»n tá»«ng bÆ°á»›c Ä‘á»ƒ tháº§y hoáº·c ngÆ°á»i má»›i cÃ³ thá»ƒ hiá»ƒu quy trÃ¬nh mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng â€” bao gá»“m cáº£ flow khi huáº¥n luyá»‡n (train) vÃ  khi Ä‘Ã¡nh giÃ¡ (evaluate), kÃ¨m tÃ³m táº¯t cÃ¡c Ä‘Æ¡n vá»‹ Ä‘o hiá»‡u suáº¥t (Accuracy / Precision / Recall / F1) theo Why / When / How / What.

### A. Flow khi huáº¥n luyá»‡n (training)
1. Chuáº©n bá»‹ dá»¯ liá»‡u
   - Nguá»“n: `data/raw/EnglishFnt/English/Fnt` (thÆ° má»¥c `SampleXXX` tÆ°Æ¡ng á»©ng tá»«ng lá»›p).
   - File liÃªn quan: `src/dataset/dataset_chars74k.py`.
2. Tiá»n xá»­ lÃ½ (transform)
   - Resize â†’ ToTensor â†’ Normalize theo ImageNet mean/std (64Ã—64).
   - File liÃªn quan: `src/transform/image_transform.py`.
3. Chia táº­p vÃ  táº¡o DataLoader
   - Chia 70%/20%/10% (train/val/test) nhÆ° trong `src/train/train.py`.
4. XÃ¢y dá»±ng mÃ´ hÃ¬nh
   - ResNetâ€‘18 tá»« `torchvision`, thay `fc` â†’ 62 output (class). File: `src/model/model_resnet18.py`.
5. Huáº¥n luyá»‡n
   - Loss: `CrossEntropyLoss`; Optimizer: `Adam` (lr=1e-3).
   - VÃ²ng láº·p: forward â†’ loss â†’ backward â†’ optimizer.step.
6. LÆ°u checkpoint
   - LÆ°u weights: `chars74k_resnet18.pth`.

_Lá»‡nh cháº¡y training (PowerShell):_
```powershell
cd "D:\Coding\Projects\learning_1st_semester_2025\AI programming\chars74k-resnet18-flask"
.venv\scripts\activate.ps1
python -m src.train.train
```

### B. Flow khi Ä‘Ã¡nh giÃ¡ (evaluation)
1. Load model checkpoint vÃ  set `model.eval()`
   - File: `src/train/evaluate_metrics.py` (script Ä‘Ã£ cÃ³) hoáº·c load trong `app/app.py` Ä‘á»ƒ inference tá»«ng áº£nh.
2. Táº¡o test DataLoader báº±ng transform test (giá»¯ cÃ¹ng split náº¿u muá»‘n tÃ¡i láº­p).
3. Forward qua toÃ n bá»™ test set (khÃ´ng grad): thu `y_true` vÃ  `y_pred`.
4. TÃ­nh metric vá»›i `sklearn.metrics` (accuracy, precision, recall, f1). CÃ³ thá»ƒ in `classification_report` vÃ  váº½ `confusion_matrix` Ä‘á»ƒ visualization.

_Lá»‡nh cháº¡y Ä‘Ã¡nh giÃ¡ (PowerShell):_
```powershell
cd "D:\Coding\Projects\learning_1st_semester_2025\AI programming\chars74k-resnet18-flask"
.venv\scripts\activate.ps1
python -m src.train.evaluate_metrics
```

---

## ğŸ“ ÄÆ¡n vá»‹ Ä‘o hiá»‡u suáº¥t â€” Why / When / How / What

1) Accuracy
- Why: Ä‘o tá»‰ lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng trÃªn tá»•ng máº«u.
- When: dÃ¹ng Ä‘á»ƒ biáº¿t tá»•ng quan khi cÃ¡c lá»›p tÆ°Æ¡ng Ä‘á»‘i cÃ¢n báº±ng.
- How: accuracy = sá»‘ dá»± Ä‘oÃ¡n Ä‘Ãºng / tá»•ng máº«u.
- What: hÃ m dÃ¹ng: `sklearn.metrics.accuracy_score(y_true, y_pred)`; report dÆ°á»›i dáº¡ng pháº§n trÄƒm.

2) Precision
- Why: Ä‘o Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c dá»± Ä‘oÃ¡n cho má»—i lá»›p (khi model nÃ³i "lÃ  X" thÃ¬ cÃ³ bao nhiÃªu lÃ  Ä‘Ãºng).
- When: quan trá»ng khi false positives tá»‘n kÃ©m.
- How: precision = TP / (TP + FP).
- What: hÃ m: `sklearn.metrics.precision_score(y_true, y_pred, average='macro')` (hoáº·c `weighted`).

3) Recall
- Why: Ä‘o kháº£ nÄƒng tÃ¬m Ä‘á»§ cÃ¡c máº«u thá»±c sá»± thuá»™c 1 lá»›p (khÃ´ng bá» sÃ³t).
- When: quan trá»ng khi false negatives tá»‘n kÃ©m.
- How: recall = TP / (TP + FN).
- What: hÃ m: `sklearn.metrics.recall_score(y_true, y_pred, average='macro')` (hoáº·c `weighted`).

4) F1â€‘score
- Why: lÃ  sá»± cÃ¢n báº±ng giá»¯a precision vÃ  recall â€” há»¯u Ã­ch khi cáº§n tradeâ€‘off.
- When: dÃ¹ng khi dataset khÃ´ng cÃ¢n báº±ng hoáº·c cáº§n 1 chá»‰ sá»‘ tÃ³m táº¯t hÆ¡n accuracy.
- How: F1 = 2 * (precision * recall) / (precision + recall).
- What: hÃ m: `sklearn.metrics.f1_score(y_true, y_pred, average='macro')` (hoáº·c `weighted`).

---

## ğŸ” Ghi chÃº quan trá»ng
- LuÃ´n nÃªu rÃµ kiá»ƒu average (`macro` / `weighted`) khi bÃ¡o Precision/Recall/F1.
- Náº¿u muá»‘n tÃ¡i láº­p káº¿t quáº£ chÃ­nh xÃ¡c, set random seed trÆ°á»›c khi chia dataset hoáº·c lÆ°u indices split.
- NÃªn kÃ¨m `confusion_matrix` (heatmap) Ä‘á»ƒ minh hoáº¡ cÃ¡c cáº·p class hay nháº§m láº«n (vÃ­ dá»¥ O â†” 0, l â†” 1).

---

## CÃ i Ä‘áº·t
```bash
python3 -m venv .venv
.venv\scripts\activate.ps1
pip install -r requirements.txt
```

## CÃ i Ä‘áº·t pytorch cÃ³ há»— trá»£ gpu
```bash
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```

---

## CÃ¡c quy trÃ¬nh

- **Dataset**
  - Thá»±c hiá»‡n trong dataset_chars74k.py
  - Load áº£nh - tráº£ vá»:
    - PIL Image
    - label (0-61)
  - khÃ´ng xá»­ lÃ½ áº£nh, transform sáº½ lÃ m
  
- **Transform**
  - Thá»±c hiá»‡n trong image_transform.py
  - Resize vá» 64x64
  - ToTensor(CHW)
  - Normalize theo ImageNet
  
- **Model (ResNet18)**
  - Load ResNet18 (pretrained)
  - Thay Ä‘á»•i fc layer tá»« 1000 â†’ 62 lá»›p
  - Forward tráº£ vá» logits [batch, 62]
  
- **Training**
  - train.py
  - Load dataset (chia 70/20/10)
  - Láº·p qua 20 epochs
    - forward â†’ loss (CrossEntropyLoss)
    - backward â†’ cáº­p nháº­t weight (Adam optimizer)
  - LÆ°u model: `chars74k_resnet18.pth`
  
- **Inference (load model + predict)**
  - Load .pth
  - Ãp dá»¥ng transform inference
  - Tráº£ vá»:
    - Top-1 prediction
    - Top-K probabilities
    
- **Flask**
  - Upload áº£nh
  - Gá»i inference module
  - Render káº¿t quáº£ dá»± Ä‘oÃ¡n

---

## MÃ´ hÃ¬nh truyá»n thá»‘ng (baseline) Ä‘á»ƒ so sÃ¡nh

### Logistic Regression (Flatten)
- **File:** baseline/logreg_flatten.py
- **PhÆ°Æ¡ng phÃ¡p:** 
  - áº¢nh â†’ grayscale â†’ resize 32Ã—32 â†’ flatten 1024 chiá»u
  - Train Logistic Regression Ä‘a lá»›p
- **Dataset:** 18,600 áº£nh (300/class)
- **Train/Test:** 80%/20%
- **Accuracy:** ~85%

### So sÃ¡nh ResNet18 vs Baseline

| Model | Accuracy | Tham sá»‘ | Thá»i gian train |
|-------|----------|---------|----------------|
| Logistic Regression | ~85% | ~63K | Nhanh |
| **ResNet18** | **91.33%** | ~11M | ~20 epochs |

**Ã nghÄ©a Baseline:**
- MÃ´ hÃ¬nh truyá»n thá»‘ng khÃ´ng há»c Ä‘Æ°á»£c Ä‘áº·c trÆ°ng phá»©c táº¡p tá»« áº£nh
- ResNet18 há»c Ä‘Æ°á»£c cáº¡nh, Ä‘Æ°á»ng cong, stroke â†’ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n 6.33%
- Trade-off: ResNet18 phá»©c táº¡p hÆ¡n nhÆ°ng cho káº¿t quáº£ tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ